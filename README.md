# LLaDA-Faithful-Summarization

**Faithfulness Evaluation of Diffusion-Based vs. Autoregressive Language Models for Abstractive Summarization**

This repository contains the codebase, evaluation scripts, and data pipelines for comparing the faithfulness of LLaDA, a diffusion-based language model, with several autoregressive models (ARMs) such as LLaMA and SmolLM, using both zero-shot and fine-tuned settings.

---

## ğŸ” Overview

Modern language models excel at fluency but often struggle with **factual faithfulness** in abstractive summarization. This project explores whether **LLaDA** can serve as a more faithful alternative to ARMs by leveraging a **diffusion-style denoising decoder** and **bidirectional context modeling**.

We build a **preference-labeled dataset** using span-level hallucination detection and fine-tune ARMs with **Direct Preference Optimization (DPO)**. Evaluations are conducted on CNN/DailyMail, SAMSum, and XSum using AlignScore and BERTScore.

---

## ğŸ“ Repository Structure

```plaintext
â”œâ”€â”€ evals/
â”‚   â”œâ”€â”€ negative_log_likelihood.py     # NLL computation
â”‚   â”œâ”€â”€ score.py                       # AlignScore & BERTScore evaluator
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ arm.py                     # Generation pipeline for ARMs
â”‚   â”‚   â””â”€â”€ llada.py                   # Generation pipeline for LLaDA
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ cnn_sys_prompt.txt
â”‚   â”‚   â”œâ”€â”€ samsum_sys_prompt.txt
â”‚   â”‚   â””â”€â”€ xsum_sys_prompt.txt
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ dpo/
â”‚   â”‚   â””â”€â”€ llada/
â”‚   â””â”€â”€ aggregated_results_per_dataset.csv  # Final evaluation metrics
â”‚
â”œâ”€â”€ sft/
â”‚   â”œâ”€â”€ preprocess.py                 # Prepares SFT training data
â”‚   â””â”€â”€ train.py                      # SFT + DPO training script
â”‚
â”œâ”€â”€ models_config.py                  # Model initialization/config
â””â”€â”€ README.md